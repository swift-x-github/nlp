{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json: 392kB [00:00, 1.92MB/s]                    \n",
      "2024-11-11 02:10:07 INFO: Downloaded file to /home/swiftx/stanza_resources/resources.json\n",
      "2024-11-11 02:10:07 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-11-11 02:10:08 INFO: File exists: /home/swiftx/stanza_resources/en/default.zip\n",
      "2024-11-11 02:10:11 INFO: Finished downloading models and saved to /home/swiftx/stanza_resources\n",
      "2024-11-11 02:10:11 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json: 392kB [00:00, 4.04MB/s]                    \n",
      "2024-11-11 02:10:12 INFO: Downloaded file to /home/swiftx/stanza_resources/resources.json\n",
      "2024-11-11 02:10:12 WARNING: Language en package default expects mwt, which has been added\n",
      "2024-11-11 02:10:12 INFO: Loading these models for language: en (English):\n",
      "=========================================\n",
      "| Processor | Package                   |\n",
      "-----------------------------------------\n",
      "| tokenize  | combined                  |\n",
      "| mwt       | combined                  |\n",
      "| ner       | ontonotes-ww-multi_charlm |\n",
      "=========================================\n",
      "\n",
      "2024-11-11 02:10:12 INFO: Using device: cpu\n",
      "2024-11-11 02:10:12 INFO: Loading: tokenize\n",
      "/home/swiftx/anaconda3/envs/stanza_env/lib/python3.8/site-packages/stanza/models/tokenization/trainer.py:82: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-11-11 02:10:12 INFO: Loading: mwt\n",
      "2024-11-11 02:10:12 INFO: Loading: ner\n",
      "2024-11-11 02:10:12 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 38.48 seconds\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import yaml\n",
    "import stanza\n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# Load configuration from YAML file\n",
    "with open('../python/config.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# Database connection parameters\n",
    "db_config = config['local_db']\n",
    "\n",
    "# Connect to the database\n",
    "conn = psycopg2.connect(\n",
    "    host=db_config['host'],\n",
    "    port=db_config['port'],\n",
    "    dbname=db_config['database'],\n",
    "    user=db_config['user'],\n",
    "    password=db_config['password']\n",
    ")\n",
    "\n",
    "# Define query parameters\n",
    "query_limit = config.get('query_stanza_limit', 2500000)\n",
    "offset = config.get('offset', 0)\n",
    "query_text = 'gold mine'\n",
    "\n",
    "# SQL query to retrieve stories containing the keyword \"gold mine\"\n",
    "story_query = f\"SELECT id, story FROM public.\\\"DJ_NEWS_STORIES\\\" WHERE story LIKE '%{query_text}%' LIMIT {query_limit} OFFSET {offset};\"\n",
    "\n",
    "# Load Stanza's English model for NER\n",
    "stanza.download('en')\n",
    "nlp = stanza.Pipeline('en', processors='tokenize,ner')\n",
    "\n",
    "# Function to clean text: removes URLs, HTML tags, punctuation, numbers, and extra whitespace\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)  # Remove URLs\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()  # Remove HTML tags\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Keep only letters and spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra whitespace\n",
    "    return text\n",
    "\n",
    "# Start processing\n",
    "start_time = time.time()\n",
    "\n",
    "# Execute query to retrieve stories\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(story_query)\n",
    "result = cursor.fetchall()\n",
    "\n",
    "# Collect unique entities and counts\n",
    "unique_entities_set = set()\n",
    "type_counts = {}\n",
    "\n",
    "# Process each story in the results\n",
    "for row in result:\n",
    "    story_id = row[0]\n",
    "    text = row[1]\n",
    "    \n",
    "    # Clean the story text\n",
    "    cleaned_text = clean_text(text)\n",
    "    \n",
    "    # Process cleaned text with Stanza NER\n",
    "    doc = nlp(cleaned_text)\n",
    "    \n",
    "    # Extract and store relevant entities\n",
    "    for ent in doc.ents:\n",
    "        if ent.type in ['ORG', 'GPE', 'PERSON']:  # Use relevant entity types\n",
    "            entity_tuple = (ent.text.strip(), ent.type)\n",
    "            if entity_tuple not in unique_entities_set:\n",
    "                unique_entities_set.add(entity_tuple)  # Add to unique entities set\n",
    "                type_counts[ent.type] = type_counts.get(ent.type, 0) + 1  # Count only unique entities\n",
    "\n",
    "# Calculate the total count of unique entities\n",
    "total_count = sum(type_counts.values())\n",
    "\n",
    "# Convert the unique entities to a DataFrame\n",
    "unique_entities_df = pd.DataFrame(sorted(unique_entities_set), columns=[\"Entity\", \"Tag\"])\n",
    "\n",
    "# Create a summary DataFrame with total unique entities per tag\n",
    "summary_df = pd.DataFrame(list(type_counts.items()), columns=[\"Tag\", \"Count\"])\n",
    "summary_df = pd.concat([summary_df, pd.DataFrame([[\"TOTAL\", total_count]], columns=[\"Tag\", \"Count\"])])\n",
    "\n",
    "# Save to Excel with only the unique entities and summary sheets\n",
    "with pd.ExcelWriter(\"recognized_entities_stanza.xlsx\", engine=\"openpyxl\") as writer:\n",
    "    unique_entities_df.to_excel(writer, sheet_name=\"Unique Entities\", index=False)\n",
    "    summary_df.to_excel(writer, sheet_name=\"Summary\", index=False)\n",
    "\n",
    "# Close the database connection\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "# Print execution time\n",
    "end_time = time.time()\n",
    "print(f\"Execution time: {end_time - start_time:.2f} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stanza_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
