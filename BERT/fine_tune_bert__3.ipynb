{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-11-13 20:19:34.522715: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-13 20:19:34.702921: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1731525574.779402 1118319 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1731525574.807124 1118319 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-13 20:19:34.969884: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import numpy as np\n",
    "from transformers import BertTokenizerFast\n",
    "from transformers import DataCollatorForTokenClassification  # This libary apply augumentation technique at runtime\n",
    "from transformers import AutoModelForTokenClassification     # This class is responsible for load model into my memory\n",
    "from datasets import Dataset, DatasetDict, ClassLabel, Sequence, Features, Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "\n",
    "# Names of mines and metals for validation\n",
    "mines = [\"Equinox Gold\"]\n",
    "metals = [\"gold\", \"silver\", \"platinum\", \"copper\", \"zinc\", \"molybdenum\", \"antimony\", \"arsenic\"]\n",
    "\n",
    "# POS, chunk, and NER tags with indices\n",
    "pos_tags = {'\"': 0, \"''\": 1, '#': 2, '$': 3, '(': 4, ')': 5, ',': 6, '.': 7, ':': 8, '``': 9,\n",
    "            'CC': 10, 'CD': 11, 'DT': 12, 'EX': 13, 'FW': 14, 'IN': 15, 'JJ': 16, 'JJR': 17, 'JJS': 18,\n",
    "            'LS': 19, 'MD': 20, 'NN': 21, 'NNP': 22, 'NNPS': 23, 'NNS': 24, 'NN|SYM': 25, 'PDT': 26,\n",
    "            'POS': 27, 'PRP': 28, 'PRP$': 29, 'RB': 30, 'RBR': 31, 'RBS': 32, 'RP': 33, 'SYM': 34,\n",
    "            'TO': 35, 'UH': 36, 'VB': 37, 'VBD': 38, 'VBG': 39, 'VBN': 40, 'VBP': 41, 'VBZ': 42,\n",
    "            'WDT': 43, 'WP': 44, 'WP$': 45, 'WRB': 46}\n",
    "\n",
    "chunk_tags = {'O': 0, 'B-ADJP': 1, 'I-ADJP': 2, 'B-ADVP': 3, 'I-ADVP': 4, 'B-CONJP': 5, 'I-CONJP': 6,\n",
    "              'B-INTJ': 7, 'I-INTJ': 8, 'B-LST': 9, 'I-LST': 10, 'B-NP': 11, 'I-NP': 12, 'B-PP': 13,\n",
    "              'I-PP': 14, 'B-PRT': 15, 'I-PRT': 16, 'B-SBAR': 17, 'I-SBAR': 18, 'B-UCP': 19, 'I-UCP': 20,\n",
    "              'B-VP': 21, 'I-VP': 22}\n",
    "\n",
    "ner_tags = {'O': 0, 'B-MINES': 1, 'I-MINES': 2, 'B-METALS': 3}\n",
    "\n",
    "# Path to the file containing news\n",
    "file_path = './files/synthetic_news_set_equinox_gold.txt'\n",
    "\n",
    "# Reading the first 20 news items\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    news_list = f.read().strip().split('\\n')[:180]  ## \n",
    "\n",
    "# Function for annotating tokens in CoNLL format\n",
    "def tag_tokens(news_list, mines, metals):\n",
    "    data = []\n",
    "    for news in news_list:\n",
    "        tokens_data = []\n",
    "        tokens = re.findall(r'\\b\\w+\\b', news)\n",
    "        i = 0\n",
    "        while i < len(tokens):\n",
    "            token = tokens[i]\n",
    "            \n",
    "            # Check for multi-word mine names\n",
    "            found_mine = None\n",
    "            for mine in mines:\n",
    "                mine_tokens = mine.split()\n",
    "                if tokens[i:i + len(mine_tokens)] == mine_tokens:\n",
    "                    found_mine = mine_tokens\n",
    "                    break\n",
    "            \n",
    "            if found_mine:\n",
    "                # Assign B-MINES to the first token and I-MINES to the rest\n",
    "                tokens_data.append([found_mine[0], pos_tags.get(\"NNP\", 21), chunk_tags.get(\"B-NP\", 11), ner_tags[\"B-MINES\"]])\n",
    "                for j in range(1, len(found_mine)):\n",
    "                    tokens_data.append([found_mine[j], pos_tags.get(\"NNP\", 21), chunk_tags.get(\"I-NP\", 12), ner_tags[\"I-MINES\"]])\n",
    "                \n",
    "                i += len(found_mine)\n",
    "                continue\n",
    "            \n",
    "            # Check for metals\n",
    "            if token in metals:\n",
    "                tokens_data.append([token, pos_tags.get(\"NN\", 21), chunk_tags.get(\"B-NP\", 11), ner_tags[\"B-METALS\"]])\n",
    "            else:\n",
    "                # If the token is neither a mine nor a metal\n",
    "                tokens_data.append([token, pos_tags.get(\"NN\", 21), chunk_tags[\"O\"], ner_tags[\"O\"]])\n",
    "            \n",
    "            i += 1\n",
    "        \n",
    "        data.append(tokens_data)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Apply the annotation function\n",
    "tagged_data = tag_tokens(news_list, mines, metals)\n",
    "\n",
    "# Function for saving data in CoNLL-2003 format\n",
    "def save_to_conll(data, file_name):\n",
    "    with open(file_name, \"w\", encoding=\"utf-8\") as f:\n",
    "        for sentence in data:\n",
    "            for token_data in sentence:\n",
    "                # Convert each element in token_data to a string and join them with spaces\n",
    "                f.write(\" \".join(map(str, token_data)) + \"\\n\")\n",
    "            f.write(\"\\n\")  # Separate sentences with an empty line\n",
    "\n",
    "# Save the data to a file\n",
    "save_to_conll(tagged_data, \"annotated_equinox_gold_news_data.conll\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
      "        num_rows: 69\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
      "        num_rows: 14\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
      "        num_rows: 16\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "import random\n",
    "\n",
    "# Function to read data from a CoNLL file and transform it into the required format\n",
    "def load_conll_data(file_path):\n",
    "    dataset = []\n",
    "    sentence = {\n",
    "        \"id\": 0,\n",
    "        \"tokens\": [],\n",
    "        \"pos_tags\": [],\n",
    "        \"chunk_tags\": [],\n",
    "        \"ner_tags\": []\n",
    "    }\n",
    "    \n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                token, pos_tag, chunk_tag, ner_tag = line.split()\n",
    "                sentence[\"tokens\"].append(token)\n",
    "                sentence[\"pos_tags\"].append(pos_tag)\n",
    "                sentence[\"chunk_tags\"].append(chunk_tag)\n",
    "                sentence[\"ner_tags\"].append(int(ner_tag))\n",
    "            else:\n",
    "                # Add the sentence to the dataset\n",
    "                dataset.append(sentence)\n",
    "                # Update the id and clear the dictionary for the next sentence\n",
    "                sentence = {\n",
    "                    \"id\": len(dataset),\n",
    "                    \"tokens\": [],\n",
    "                    \"pos_tags\": [],\n",
    "                    \"chunk_tags\": [],\n",
    "                    \"ner_tags\": []\n",
    "                }\n",
    "    \n",
    "    # Add the last sentence if the file does not end with an empty line\n",
    "    if sentence[\"tokens\"]:\n",
    "        dataset.append(sentence)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# Load the data\n",
    "file_path = \"annotated_equinox_gold_news_data.conll\"\n",
    "data = load_conll_data(file_path)\n",
    "\n",
    "# Shuffle the data and split it into train, validation, and test sets\n",
    "random.shuffle(data)\n",
    "train_size = int(0.7 * len(data))\n",
    "valid_size = int(0.15 * len(data))\n",
    "\n",
    "train_data = data[:train_size]\n",
    "validation_data = data[train_size:train_size + valid_size]\n",
    "test_data = data[train_size + valid_size:]\n",
    "\n",
    "# Convert the data into the DatasetDict format\n",
    "ner_data = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(pd.DataFrame(train_data)),\n",
    "    \"validation\": Dataset.from_pandas(pd.DataFrame(validation_data)),\n",
    "    \"test\": Dataset.from_pandas(pd.DataFrame(test_data))\n",
    "})\n",
    "\n",
    "# Display the structure of the DatasetDict\n",
    "print(ner_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 79, 'tokens': [], 'pos_tags': [], 'chunk_tags': [], 'ner_tags': []}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# structure of train data\n",
    "ner_data['train'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 80, 20, 84, 18, 29, 50, 86, 66, 97, 67, 78, 14, 3]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#structure of test data\n",
    "ner_data['validation']['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 88,\n",
       " 'tokens': ['Equinox',\n",
       "  'Gold',\n",
       "  'Launches',\n",
       "  'Carbon',\n",
       "  'Offset',\n",
       "  'Program',\n",
       "  'with',\n",
       "  'Global',\n",
       "  'Reach',\n",
       "  'Partnering',\n",
       "  'with',\n",
       "  'global',\n",
       "  'environmental',\n",
       "  'groups',\n",
       "  'Equinox',\n",
       "  'Gold',\n",
       "  'is',\n",
       "  'investing',\n",
       "  'in',\n",
       "  'carbon',\n",
       "  'offset',\n",
       "  'projects',\n",
       "  'to',\n",
       "  'neutralize',\n",
       "  'its',\n",
       "  'emissions'],\n",
       " 'pos_tags': ['22',\n",
       "  '22',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '22',\n",
       "  '22',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21'],\n",
       " 'chunk_tags': ['11',\n",
       "  '12',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '11',\n",
       "  '12',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0'],\n",
       " 'ner_tags': [1,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_data['test'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_data['validation'].features['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_data['train'].features[\"ner_tags\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_data['train'].features[\"ner_tags\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = ner_data['train'][0]\n",
    "tokenized_input = tokenizer(example_text['tokens'],is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "word_ids = tokenized_input.word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 1041, 12519, 11636, 2751, 6753, 2334, 2495, 2007, 2082, 6502, 27001, 2007, 2334, 6867, 1041, 12519, 11636, 2751, 2003, 4804, 1996, 2810, 1997, 2047, 2816, 2379, 2049, 3136, 2000, 5335, 2334, 2495, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "\n",
      "['[CLS]', 'e', '##quin', '##ox', 'gold', 'supports', 'local', 'education', 'with', 'school', 'infrastructure', 'partnering', 'with', 'local', 'governments', 'e', '##quin', '##ox', 'gold', 'is', 'funding', 'the', 'construction', 'of', 'new', 'schools', 'near', 'its', 'operations', 'to', 'improve', 'local', 'education', '[SEP]']\n",
      "\n",
      "\n",
      "[None, 0, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 12, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, None]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_input)\n",
    "print(\"\\n\")\n",
    "print(tokens)\n",
    "print(\"\\n\")\n",
    "print(word_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the tokens is : 34\n",
      "Length of the ner tags is: 28\n"
     ]
    }
   ],
   "source": [
    "print(f'Length of the tokens is : {len(tokens)}')\n",
    "print(f'Length of the ner tags is: {len(ner_data[\"train\"][0][\"ner_tags\"])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples, label_all_tokens=True):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        # word_ids() => Return a list mapping the tokens\n",
    "        # to their actual word in the initial sentence.\n",
    "        # It Returns a list indicating the word corresponding to each token.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        # Special tokens like `` and `<\\s>` are originally mapped to None\n",
    "        # We need to set the label to -100 so they are automatically ignored in the loss function.\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                # set –100 as the label for these special tokens\n",
    "                label_ids.append(-100)\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                # if current word_idx is != prev then its the most regular case\n",
    "                # and add the corresponding token\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                # to take care of sub-words which have the same word_idx\n",
    "                # set -100 as well for them, but only if label_all_tokens == False\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "                # mask the subword representations after the first subword\n",
    "\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': [38],\n",
       " 'tokens': [['Equinox',\n",
       "   'Gold',\n",
       "   's',\n",
       "   'Safety',\n",
       "   'Record',\n",
       "   'Reaches',\n",
       "   'New',\n",
       "   'Heights',\n",
       "   'in',\n",
       "   '2023',\n",
       "   'Equinox',\n",
       "   'Gold',\n",
       "   'achieved',\n",
       "   'its',\n",
       "   'safest',\n",
       "   'year',\n",
       "   'on',\n",
       "   'record',\n",
       "   'with',\n",
       "   'significant',\n",
       "   'reductions',\n",
       "   'in',\n",
       "   'workplace',\n",
       "   'injuries',\n",
       "   'due',\n",
       "   'to',\n",
       "   'its',\n",
       "   'robust',\n",
       "   'safety',\n",
       "   'protocols']],\n",
       " 'pos_tags': [['22',\n",
       "   '22',\n",
       "   '21',\n",
       "   '21',\n",
       "   '21',\n",
       "   '21',\n",
       "   '21',\n",
       "   '21',\n",
       "   '21',\n",
       "   '21',\n",
       "   '22',\n",
       "   '22',\n",
       "   '21',\n",
       "   '21',\n",
       "   '21',\n",
       "   '21',\n",
       "   '21',\n",
       "   '21',\n",
       "   '21',\n",
       "   '21',\n",
       "   '21',\n",
       "   '21',\n",
       "   '21',\n",
       "   '21',\n",
       "   '21',\n",
       "   '21',\n",
       "   '21',\n",
       "   '21',\n",
       "   '21',\n",
       "   '21']],\n",
       " 'chunk_tags': [['11',\n",
       "   '12',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '11',\n",
       "   '12',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0']],\n",
       " 'ner_tags': [[1,\n",
       "   2,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   1,\n",
       "   2,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0]]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_data['train'][1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 1041, 12519, 11636, 2751, 1055, 3808, 2501, 6561, 2047, 7535, 1999, 16798, 2509, 1041, 12519, 11636, 2751, 4719, 2049, 3647, 3367, 2095, 2006, 2501, 2007, 3278, 25006, 1999, 16165, 6441, 2349, 2000, 2049, 15873, 3808, 16744, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[-100, 1, 1, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]]}\n"
     ]
    }
   ],
   "source": [
    "q = tokenize_and_align_labels(ner_data['train'][1:2])\n",
    "print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]___________________________________ -100\n",
      "e_______________________________________ 1\n",
      "##quin__________________________________ 1\n",
      "##ox____________________________________ 1\n",
      "gold____________________________________ 2\n",
      "s_______________________________________ 0\n",
      "safety__________________________________ 0\n",
      "record__________________________________ 0\n",
      "reaches_________________________________ 0\n",
      "new_____________________________________ 0\n",
      "heights_________________________________ 0\n",
      "in______________________________________ 0\n",
      "202_____________________________________ 0\n",
      "##3_____________________________________ 0\n",
      "e_______________________________________ 1\n",
      "##quin__________________________________ 1\n",
      "##ox____________________________________ 1\n",
      "gold____________________________________ 2\n",
      "achieved________________________________ 0\n",
      "its_____________________________________ 0\n",
      "safe____________________________________ 0\n",
      "##st____________________________________ 0\n",
      "year____________________________________ 0\n",
      "on______________________________________ 0\n",
      "record__________________________________ 0\n",
      "with____________________________________ 0\n",
      "significant_____________________________ 0\n",
      "reductions______________________________ 0\n",
      "in______________________________________ 0\n",
      "workplace_______________________________ 0\n",
      "injuries________________________________ 0\n",
      "due_____________________________________ 0\n",
      "to______________________________________ 0\n",
      "its_____________________________________ 0\n",
      "robust__________________________________ 0\n",
      "safety__________________________________ 0\n",
      "protocols_______________________________ 0\n",
      "[SEP]___________________________________ -100\n"
     ]
    }
   ],
   "source": [
    "for token, label in zip(tokenizer.convert_ids_to_tokens(q[\"input_ids\"][0]),q[\"labels\"][0]):\n",
    "    print(f\"{token:_<40} {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/69 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 69/69 [00:00<00:00, 2732.96 examples/s]\n",
      "Map: 100%|██████████| 14/14 [00:00<00:00, 2399.78 examples/s]\n",
      "Map: 100%|██████████| 16/16 [00:00<00:00, 1735.96 examples/s]\n"
     ]
    }
   ],
   "source": [
    "## Applying on entire data\n",
    "tokenized_datasets = ner_data.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 38,\n",
       " 'tokens': ['Equinox',\n",
       "  'Gold',\n",
       "  's',\n",
       "  'Safety',\n",
       "  'Record',\n",
       "  'Reaches',\n",
       "  'New',\n",
       "  'Heights',\n",
       "  'in',\n",
       "  '2023',\n",
       "  'Equinox',\n",
       "  'Gold',\n",
       "  'achieved',\n",
       "  'its',\n",
       "  'safest',\n",
       "  'year',\n",
       "  'on',\n",
       "  'record',\n",
       "  'with',\n",
       "  'significant',\n",
       "  'reductions',\n",
       "  'in',\n",
       "  'workplace',\n",
       "  'injuries',\n",
       "  'due',\n",
       "  'to',\n",
       "  'its',\n",
       "  'robust',\n",
       "  'safety',\n",
       "  'protocols'],\n",
       " 'pos_tags': ['22',\n",
       "  '22',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '22',\n",
       "  '22',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21'],\n",
       " 'chunk_tags': ['11',\n",
       "  '12',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '11',\n",
       "  '12',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0'],\n",
       " 'ner_tags': [1,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'input_ids': [101,\n",
       "  1041,\n",
       "  12519,\n",
       "  11636,\n",
       "  2751,\n",
       "  1055,\n",
       "  3808,\n",
       "  2501,\n",
       "  6561,\n",
       "  2047,\n",
       "  7535,\n",
       "  1999,\n",
       "  16798,\n",
       "  2509,\n",
       "  1041,\n",
       "  12519,\n",
       "  11636,\n",
       "  2751,\n",
       "  4719,\n",
       "  2049,\n",
       "  3647,\n",
       "  3367,\n",
       "  2095,\n",
       "  2006,\n",
       "  2501,\n",
       "  2007,\n",
       "  3278,\n",
       "  25006,\n",
       "  1999,\n",
       "  16165,\n",
       "  6441,\n",
       "  2349,\n",
       "  2000,\n",
       "  2049,\n",
       "  15873,\n",
       "  3808,\n",
       "  16744,\n",
       "  102],\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'labels': [-100,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets['train'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Defining model\n",
    "ner_model = AutoModelForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('4.46.1', '1.1.0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install accelerate>=1\n",
    "import accelerate\n",
    "import transformers\n",
    "\n",
    "transformers.__version__, accelerate.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tf-keras in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (2.18.0)\n",
      "Requirement already satisfied: tensorflow<2.19,>=2.18 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from tf-keras) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: packaging in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (5.28.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.67.1)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (3.6.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.0.2)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (0.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (0.37.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow<2.19,>=2.18->tf-keras) (0.38.4)\n",
      "Requirement already satisfied: rich in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (13.9.3)\n",
      "Requirement already satisfied: namex in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (0.0.8)\n",
      "Requirement already satisfied: optree in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (0.13.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (3.0.6)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (8.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (2.18.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (3.20.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install tf-keras\n",
    "#Define training args\n",
    "from transformers import TrainingArguments, Trainer\n",
    "args = TrainingArguments(\n",
    "\"test-ner\",\n",
    "evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=6,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U datasets evaluate\n",
    "from evaluate import load\n",
    "metric = load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassLabel(names=['O', 'B-MINES', 'I-MINES', 'B-METALS'], id=None)\n"
     ]
    }
   ],
   "source": [
    "print(ner_data[\"train\"].features[\"ner_tags\"].feature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Casting the dataset: 100%|██████████| 69/69 [00:00<00:00, 6196.75 examples/s]\n",
      "Casting the dataset: 100%|██████████| 14/14 [00:00<00:00, 3905.83 examples/s]\n",
      "Casting the dataset: 100%|██████████| 16/16 [00:00<00:00, 6947.81 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'B-MINES', 'I-MINES', 'B-METALS']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['O', 'B-MINES', 'I-MINES', 'B-METALS']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import DatasetDict, ClassLabel, Sequence, Features, Value\n",
    "\n",
    "# Defining labels for ner_tags\n",
    "ner_label_names = ['O', 'B-MINES', 'I-MINES', 'B-METALS']\n",
    "features = Features({\n",
    "    \"id\": Value(\"int64\"),\n",
    "    \"tokens\": Sequence(Value(\"string\")),\n",
    "    \"pos_tags\": Sequence(Value(\"int64\")),\n",
    "    \"chunk_tags\": Sequence(Value(\"int64\")),\n",
    "    \"ner_tags\": Sequence(ClassLabel(names=ner_label_names))\n",
    "})\n",
    "\n",
    "# Converting ner_tags to ClassLabel after dataset creation\n",
    "ner_data[\"train\"] = ner_data[\"train\"].cast(features)\n",
    "ner_data[\"validation\"] = ner_data[\"validation\"].cast(features)\n",
    "ner_data[\"test\"] = ner_data[\"test\"].cast(features)\n",
    "\n",
    "# Now checking the labels\n",
    "print(ner_data[\"train\"].features[\"ner_tags\"].feature.names)\n",
    "label_list = ner_data[\"train\"].features[\"ner_tags\"].feature.names\n",
    "label_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    pred_logits, labels = eval_preds\n",
    "    print(eval_preds)\n",
    "\n",
    "    pred_logits = np.argmax(pred_logits, axis=2)\n",
    "    # the logits and the probabilities are in the same order,\n",
    "    # so we don’t need to apply the softmax\n",
    "\n",
    "    # We remove all the values where the label is -100\n",
    "    predictions = [\n",
    "        [label_list[eval_preds] for (eval_preds, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(pred_logits, labels)\n",
    "    ]\n",
    "\n",
    "    true_labels = [\n",
    "      [label_list[l] for (eval_preds, l) in zip(prediction, label) if l != -100]\n",
    "       for prediction, label in zip(pred_logits, labels)\n",
    "   ]\n",
    "    results = metric.compute(predictions=predictions, references=true_labels)\n",
    "\n",
    "    return {\n",
    "          \"precision\": results[\"overall_precision\"],\n",
    "          \"recall\": results[\"overall_recall\"],\n",
    "          \"f1\": results[\"overall_f1\"],\n",
    "          \"accuracy\": results[\"overall_accuracy\"],\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1118319/2818633497.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "   ner_model,\n",
    "   args,\n",
    "   train_dataset=tokenized_datasets[\"train\"],\n",
    "   eval_dataset=tokenized_datasets[\"validation\"],\n",
    "   data_collator=data_collator,\n",
    "   tokenizer=tokenizer,\n",
    "   compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                               \n",
      "\u001b[A                                           \n",
      "\n",
      " 52%|█████▏    | 31/60 [06:49<08:04, 16.72s/it]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x7e01945b5910>\n",
      "{'eval_loss': 0.0028097445610910654, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 0.7337, 'eval_samples_per_second': 19.082, 'eval_steps_per_second': 1.363, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                               \n",
      "\u001b[A                                           \n",
      "\n",
      " 52%|█████▏    | 31/60 [07:01<08:04, 16.72s/it]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x7e00e8bac5e0>\n",
      "{'eval_loss': 0.0017707530641928315, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 0.6572, 'eval_samples_per_second': 21.301, 'eval_steps_per_second': 1.522, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                               \n",
      "\n",
      "\u001b[A\u001b[A                               \n",
      " 52%|█████▏    | 31/60 [07:13<08:04, 16.72s/it]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x7e026ab692b0>\n",
      "{'eval_loss': 0.0012824329314753413, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 0.7112, 'eval_samples_per_second': 19.685, 'eval_steps_per_second': 1.406, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                               \n",
      "\n",
      "\u001b[A\u001b[A                               \n",
      " 52%|█████▏    | 31/60 [07:27<08:04, 16.72s/it]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x7e026ab4dfd0>\n",
      "{'eval_loss': 0.0010441734921187162, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 0.8498, 'eval_samples_per_second': 16.474, 'eval_steps_per_second': 1.177, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                               \n",
      "\n",
      "\u001b[A\u001b[A                               \n",
      " 52%|█████▏    | 31/60 [07:41<08:04, 16.72s/it]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x7e026a93fe50>\n",
      "{'eval_loss': 0.0009387984173372388, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 0.7683, 'eval_samples_per_second': 18.223, 'eval_steps_per_second': 1.302, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                               \n",
      "\u001b[A                                            \n",
      "\n",
      " 52%|█████▏    | 31/60 [07:55<08:04, 16.72s/it]\n",
      "\u001b[A\n",
      "                                               \n",
      "100%|██████████| 18/18 [02:50<00:00,  9.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x7e00e8cb7be0>\n",
      "{'eval_loss': 0.0009072309476323426, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_accuracy': 1.0, 'eval_runtime': 0.593, 'eval_samples_per_second': 23.609, 'eval_steps_per_second': 1.686, 'epoch': 6.0}\n",
      "{'train_runtime': 170.7497, 'train_samples_per_second': 2.425, 'train_steps_per_second': 0.105, 'train_loss': 0.0052255164417955614, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=18, training_loss=0.0052255164417955614, metrics={'train_runtime': 170.7497, 'train_samples_per_second': 2.425, 'train_steps_per_second': 0.105, 'total_flos': 9503820679632.0, 'train_loss': 0.0052255164417955614, 'epoch': 6.0})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save model\n",
    "ner_model.save_pretrained(\"ner_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tokenizer/tokenizer_config.json',\n",
       " 'tokenizer/special_tokens_map.json',\n",
       " 'tokenizer/vocab.txt',\n",
       " 'tokenizer/added_tokens.json',\n",
       " 'tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Save tokenizer\n",
    "tokenizer.save_pretrained(\"tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    str(i): label for i,label in enumerate(label_list)\n",
    "}\n",
    "label2id = {\n",
    "    label: str(i) for i,label in enumerate(label_list)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': 'O', '1': 'B-MINES', '2': 'I-MINES', '3': 'B-METALS'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': '0', 'B-MINES': '1', 'I-MINES': '2', 'B-METALS': '3'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = json.load(open(\"ner_model/config.json\"))\n",
    "config[\"id2label\"] = id2label\n",
    "config[\"label2id\"] = label2id\n",
    "json.dump(config, open(\"ner_model/config.json\",\"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fine_tuned = AutoModelForTokenClassification.from_pretrained(\"ner_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'B-MINES', 'score': np.float32(0.998533), 'index': 1, 'word': 'e', 'start': 0, 'end': 1}, {'entity': 'B-MINES', 'score': np.float32(0.998447), 'index': 2, 'word': '##quin', 'start': 1, 'end': 5}, {'entity': 'B-MINES', 'score': np.float32(0.99803966), 'index': 3, 'word': '##ox', 'start': 5, 'end': 7}, {'entity': 'B-MINES', 'score': np.float32(0.4476207), 'index': 4, 'word': 'goo', 'start': 8, 'end': 11}]\n"
     ]
    }
   ],
   "source": [
    "nlp = pipeline(\"ner\",model=model_fine_tuned,tokenizer=tokenizer)\n",
    "example = \"Equinox Goold Corp. Is Maintained at Sector Perform by National Bank. Ratings actions from Baystreet: http://www.baystreet.ca (END) Dow Jones Newswires  August 05, 2022 12:33 ET (16:33 GMT)\"\n",
    "ner_results = nlp(example)\n",
    "print(ner_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сущность: equinox, Тег: B-MINES, Начало: 0, Конец: 7, Оценка: 1.00\n",
      "Сущность: goo, Тег: B-MINES, Начало: 8, Конец: 11, Оценка: 0.45\n"
     ]
    }
   ],
   "source": [
    "# Processing and merging subwords with added score\n",
    "entities = []\n",
    "current_entity = \"\"\n",
    "current_tag = None\n",
    "current_start = None\n",
    "current_score_sum = 0.0\n",
    "current_score_count = 0\n",
    "\n",
    "for item in ner_results:\n",
    "    word = item['word']\n",
    "    tag = item['entity']\n",
    "    score = item['score']\n",
    "    \n",
    "    # Start of a new word or entity\n",
    "    if word.startswith(\"##\"):\n",
    "        # Remove \"##\" and append it to the current word\n",
    "        current_entity += word[2:]\n",
    "        current_score_sum += score\n",
    "        current_score_count += 1\n",
    "    else:\n",
    "        # Save the current entity if it is completed\n",
    "        if current_entity:\n",
    "            avg_score = current_score_sum / current_score_count if current_score_count > 0 else 0\n",
    "            entities.append({\n",
    "                \"entity\": current_tag,\n",
    "                \"word\": current_entity,\n",
    "                \"start\": current_start,\n",
    "                \"end\": item['start'] - 1,  # End of the previous word\n",
    "                \"score\": avg_score\n",
    "            })\n",
    "        \n",
    "        # Initialize a new word\n",
    "        current_entity = word\n",
    "        current_tag = tag\n",
    "        current_start = item['start']\n",
    "        current_score_sum = score\n",
    "        current_score_count = 1\n",
    "\n",
    "# Add the last entity\n",
    "if current_entity:\n",
    "    avg_score = current_score_sum / current_score_count if current_score_count > 0 else 0\n",
    "    entities.append({\n",
    "        \"entity\": current_tag,\n",
    "        \"word\": current_entity,\n",
    "        \"start\": current_start,\n",
    "        \"end\": item['end'],\n",
    "        \"score\": avg_score\n",
    "    })\n",
    "\n",
    "# Output the merged entities\n",
    "for entity in entities:\n",
    "    print(f\"Entity: {entity['word']}, Tag: {entity['entity']}, Start: {entity['start']}, End: {entity['end']}, Score: {entity['score']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (4.46.1)\n",
      "Requirement already satisfied: psycopg2 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (2.9.10)\n",
      "Requirement already satisfied: pandas in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (2.2.3)\n",
      "Requirement already satisfied: openpyxl in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (3.1.5)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (4.12.3)\n",
      "Requirement already satisfied: filelock in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from transformers) (0.26.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: et-xmlfile in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from openpyxl) (2.0.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from beautifulsoup4) (2.6)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from requests->transformers) (2024.8.30)\n",
      "Execution time: 64.86 seconds\n"
     ]
    }
   ],
   "source": [
    "# Install necessary packages if not already installed\n",
    "!pip install transformers psycopg2 pandas openpyxl beautifulsoup4\n",
    "\n",
    "import psycopg2\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "\n",
    "# Load configuration from YAML file\n",
    "with open('config.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# Database connection parameters\n",
    "db_config = config['local_db']  # Adjust to 'remote_db' if needed\n",
    "\n",
    "# Connect to the database\n",
    "conn = psycopg2.connect(\n",
    "    host=db_config['host'],\n",
    "    port=db_config['port'],\n",
    "    dbname=db_config['database'],\n",
    "    user=db_config['user'],\n",
    "    password=db_config['password']\n",
    ")\n",
    "\n",
    "# Define the specific query text and SQL query\n",
    "query_text_1 = 'gold mine'\n",
    "query_text_2 = 'Equinox Gold'\n",
    "\n",
    "# SQL query to retrieve stories containing both \"gold mine\" and \"Equinox Gold\"\n",
    "story_query = f\"\"\"\n",
    "    SELECT id, story \n",
    "    FROM public.\"DJ_NEWS_STORIES\" \n",
    "    WHERE story LIKE '%{query_text_1}%' \n",
    "    AND story LIKE '%{query_text_2}%';\n",
    "\"\"\"\n",
    "\n",
    "# Load the fine-tuned model and tokenizer for NER\n",
    "model_fine_tuned = AutoModelForTokenClassification.from_pretrained(\"ner_model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tokenizer\")\n",
    "nlp = pipeline(\"ner\", model=model_fine_tuned, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "\n",
    "# Function to clean text: removes URLs, HTML tags, punctuation, numbers, and extra whitespace\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)  # Remove URLs\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()  # Remove HTML tags\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove non-letter characters\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra whitespace\n",
    "    return text\n",
    "\n",
    "# Start processing\n",
    "start_time = time.time()\n",
    "\n",
    "# Execute query to retrieve stories\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(story_query)\n",
    "result = cursor.fetchall()\n",
    "\n",
    "# Collect entities and counts\n",
    "unique_entities_set = set()\n",
    "all_entities = set()\n",
    "type_counts = {\"MINES\": 0, \"METAL\": 0}\n",
    "total_count = 0\n",
    "\n",
    "# Process each story in the results\n",
    "for row in result:\n",
    "    story_id = row[0]\n",
    "    text = row[1]\n",
    "    \n",
    "    # Clean the story text\n",
    "    cleaned_text = clean_text(text)\n",
    "    \n",
    "    # Process cleaned text with the fine-tuned NER pipeline\n",
    "    ner_results = nlp(cleaned_text)\n",
    "    \n",
    "    # Extract only 'MINES' and 'METAL' entities\n",
    "    for ent in ner_results:\n",
    "        entity_text = ent['word']\n",
    "        entity_type = ent['entity_group']\n",
    "        if entity_type in ['MINES', 'METAL']:  # Only process 'MINES' and 'METAL' entities\n",
    "            entity_tuple = (entity_text, entity_type, story_id)\n",
    "            all_entities.add(entity_tuple)\n",
    "            unique_entities_set.add((entity_text, entity_type))\n",
    "            \n",
    "            # Update counts for summary\n",
    "            type_counts[entity_type] += 1\n",
    "            total_count += 1\n",
    "\n",
    "# Convert sets to sorted DataFrames for saving to Excel\n",
    "unique_entities_df = pd.DataFrame(sorted(unique_entities_set), columns=[\"Entity\", \"Tag\"])\n",
    "all_entities_df = pd.DataFrame(sorted(all_entities), columns=[\"Entity\", \"Tag\", \"Story ID\"])\n",
    "summary_df = pd.DataFrame(list(type_counts.items()), columns=[\"Tag\", \"Count\"])\n",
    "summary_df = pd.concat([summary_df, pd.DataFrame([[\"TOTAL\", total_count]], columns=[\"Tag\", \"Count\"])])\n",
    "\n",
    "# Save to Excel with multiple sheets\n",
    "with pd.ExcelWriter(\"recognized_mines_metals_entities.xlsx\", engine=\"openpyxl\") as writer:\n",
    "    unique_entities_df.to_excel(writer, sheet_name=\"Unique Entities\", index=False)\n",
    "    all_entities_df.to_excel(writer, sheet_name=\"All Entities with Story ID\", index=False)\n",
    "    summary_df.to_excel(writer, sheet_name=\"Summary\", index=False)\n",
    "\n",
    "# Close the database connection\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "# Print execution time\n",
    "end_time = time.time()\n",
    "print(f\"Execution time: {end_time - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# List to store results for Excel\n",
    "mines_data = []\n",
    "\n",
    "# Process each story in the results\n",
    "for row in result:\n",
    "    story_id = row[0]\n",
    "    text = row[1]\n",
    "    \n",
    "    # Clean the story text\n",
    "    cleaned_text = clean_text(text)\n",
    "    \n",
    "    # Process cleaned text with fine-tuned BERT NER\n",
    "    ner_results = nlp(cleaned_text)\n",
    "    \n",
    "    # Track individual entities within a single story\n",
    "    current_entity = \"\"\n",
    "    score_total = 0.0\n",
    "    entity_segments = []\n",
    "\n",
    "    for ent in ner_results:\n",
    "        if ent['entity'] == 'B-MINES' and ent['score'] > 0.01:\n",
    "            # Save current entity if it's not empty\n",
    "            if current_entity:\n",
    "                mines_data.append({\n",
    "                    \"Story ID\": story_id,\n",
    "                    \"Entity\": current_entity,\n",
    "                    \"Score\": round(score_total, 2)\n",
    "                })\n",
    "            # Start a new entity\n",
    "            current_entity = ent['word'].replace(\"##\", \"\")\n",
    "            score_total = ent['score']\n",
    "        \n",
    "        elif ent['entity'] == 'I-MINES' and ent['score'] > 0.01:\n",
    "            # Continue with the same entity\n",
    "            current_entity += ent['word'].replace(\"##\", \"\")\n",
    "            score_total += ent['score']\n",
    "\n",
    "    # Append the final entity after the loop if it exists\n",
    "    if current_entity:\n",
    "        mines_data.append({\n",
    "            \"Story ID\": story_id,\n",
    "            \"Entity\": current_entity,\n",
    "            \"Score\": round(score_total, 2)\n",
    "        })\n",
    "\n",
    "# Convert results to a DataFrame for saving to Excel\n",
    "mines_df = pd.DataFrame(mines_data)\n",
    "\n",
    "# Save results to Excel\n",
    "with pd.ExcelWriter(\"mines_entities_results.xlsx\", engine=\"openpyxl\") as writer:\n",
    "    mines_df.to_excel(writer, sheet_name=\"MINES Entities\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Уникальные ID новостей:\n",
      "[2469129 2480810 2480823 2631654 2631656 2692968 2692973 2692987 2692989\n",
      " 2719306 2719308 2719350   13614   13645   61756  155272  807117 1303430\n",
      " 1303477 1681402 1832460 1837994 1897329 2426242 2426243 2521095 2521100\n",
      "   98705   98706  622718  781141  781146  964239  964240  989393  989493\n",
      "  996781  996784 1009534 1009849 1144609 1144619 1288536 1288537 1472693\n",
      " 1472689 1611925 1611930 1830848 1830917 2074020 2147733 2147739 2271646\n",
      " 2290596 2290598 2357949 2357971 2455010 2455013 2598547 2611695  191067\n",
      "  191068  241901  285979  285987  684685  684784  781165 1058634 1058635\n",
      " 1058639 1058645 1420884 1420892 1500363 1500368 1500369 1500372 1834646\n",
      " 1860984 1967874 1967877 2025197 2025201]\n",
      "Общее количество уникальных ID новостей: 86\n"
     ]
    }
   ],
   "source": [
    "# Retrieving unique values from the 'Story ID' column in mines_df\n",
    "unique_story_ids = mines_df['Story ID'].unique()\n",
    "\n",
    "# Printing unique values and their total count\n",
    "print(\"Unique Story IDs:\")\n",
    "print(unique_story_ids)\n",
    "print(f\"Total number of unique Story IDs: {len(unique_story_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Story ID</th>\n",
       "      <th>Entity</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2469129</td>\n",
       "      <td>e</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2469129</td>\n",
       "      <td>quin</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2469129</td>\n",
       "      <td>oxgold</td>\n",
       "      <td>1.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2480810</td>\n",
       "      <td>e</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2480810</td>\n",
       "      <td>quin</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1708</th>\n",
       "      <td>2025197</td>\n",
       "      <td>quin</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1709</th>\n",
       "      <td>2025197</td>\n",
       "      <td>oxgold</td>\n",
       "      <td>1.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1710</th>\n",
       "      <td>2025201</td>\n",
       "      <td>e</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1711</th>\n",
       "      <td>2025201</td>\n",
       "      <td>quin</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1712</th>\n",
       "      <td>2025201</td>\n",
       "      <td>oxgold</td>\n",
       "      <td>1.88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1713 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Story ID  Entity  Score\n",
       "0      2469129       e   1.00\n",
       "1      2469129    quin   1.00\n",
       "2      2469129  oxgold   1.79\n",
       "3      2480810       e   1.00\n",
       "4      2480810    quin   1.00\n",
       "...        ...     ...    ...\n",
       "1708   2025197    quin   1.00\n",
       "1709   2025197  oxgold   1.88\n",
       "1710   2025201       e   1.00\n",
       "1711   2025201    quin   1.00\n",
       "1712   2025201  oxgold   1.88\n",
       "\n",
       "[1713 rows x 3 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mines_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
