{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import numpy as np\n",
    "from transformers import BertTokenizerFast\n",
    "from transformers import DataCollatorForTokenClassification  # This libary apply augumentation technique at runtime\n",
    "from transformers import AutoModelForTokenClassification     # This class is responsible for load model into my memory\n",
    "from datasets import Dataset, DatasetDict, ClassLabel, Sequence, Features, Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "\n",
    "# Names of mines and metals for validation\n",
    "mines = [\"Aguas Calientes\", \"Amiches\", \"Arroyo Verde\", \"Aylen\", \"Boleadora\", \"Cachi\", \"Calcatreu\",\n",
    "         \"Canadon Langostura\", \"Cerro Negro\", \"Newmont Corporation\", \"Latin Metals Inc\",\n",
    "         \"International Iconic Gold Exploration Corp\", \"Entropy Resources\",\n",
    "         \"Aldebaran Resources Inc\", \"NewPeak Metals Limited\", \"Patagonia Gold Corp\", \"E2 Metals Limited\"]\n",
    "metals = [\"gold\", \"silver\", \"platinum\", \"copper\", \"zinc\", \"molybdenum\", \"antimony\", \"arsenic\"]\n",
    "\n",
    "# POS, chunk, and NER tags with indices\n",
    "pos_tags = {'\"': 0, \"''\": 1, '#': 2, '$': 3, '(': 4, ')': 5, ',': 6, '.': 7, ':': 8, '``': 9,\n",
    "            'CC': 10, 'CD': 11, 'DT': 12, 'EX': 13, 'FW': 14, 'IN': 15, 'JJ': 16, 'JJR': 17, 'JJS': 18,\n",
    "            'LS': 19, 'MD': 20, 'NN': 21, 'NNP': 22, 'NNPS': 23, 'NNS': 24, 'NN|SYM': 25, 'PDT': 26,\n",
    "            'POS': 27, 'PRP': 28, 'PRP$': 29, 'RB': 30, 'RBR': 31, 'RBS': 32, 'RP': 33, 'SYM': 34,\n",
    "            'TO': 35, 'UH': 36, 'VB': 37, 'VBD': 38, 'VBG': 39, 'VBN': 40, 'VBP': 41, 'VBZ': 42,\n",
    "            'WDT': 43, 'WP': 44, 'WP$': 45, 'WRB': 46}\n",
    "\n",
    "chunk_tags = {'O': 0, 'B-ADJP': 1, 'I-ADJP': 2, 'B-ADVP': 3, 'I-ADVP': 4, 'B-CONJP': 5, 'I-CONJP': 6,\n",
    "              'B-INTJ': 7, 'I-INTJ': 8, 'B-LST': 9, 'I-LST': 10, 'B-NP': 11, 'I-NP': 12, 'B-PP': 13,\n",
    "              'I-PP': 14, 'B-PRT': 15, 'I-PRT': 16, 'B-SBAR': 17, 'I-SBAR': 18, 'B-UCP': 19, 'I-UCP': 20,\n",
    "              'B-VP': 21, 'I-VP': 22}\n",
    "\n",
    "ner_tags = {'O': 0, 'B-MINES': 1, 'I-MINES': 2, 'B-METALS': 3}\n",
    "\n",
    "# Path to the file containing news\n",
    "file_path = './files/synthetic_news_set.txt'\n",
    "\n",
    "# Reading the first 20 news items\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    news_list = f.read().strip().split('\\n')[:180]   ## \n",
    "\n",
    "# Function to annotate tokens in CoNLL format\n",
    "def tag_tokens(news_list, mines, metals):\n",
    "    data = []\n",
    "    for news in news_list:\n",
    "        tokens_data = []\n",
    "        tokens = re.findall(r'\\b\\w+\\b', news)\n",
    "        i = 0\n",
    "        while i < len(tokens):\n",
    "            token = tokens[i]\n",
    "            \n",
    "            # Checking for multi-word mine names\n",
    "            found_mine = None\n",
    "            for mine in mines:\n",
    "                mine_tokens = mine.split()\n",
    "                if tokens[i:i + len(mine_tokens)] == mine_tokens:\n",
    "                    found_mine = mine_tokens\n",
    "                    break\n",
    "            \n",
    "            if found_mine:\n",
    "                # Assigning B-MINES to the first token and I-MINES to the rest\n",
    "                tokens_data.append([found_mine[0], pos_tags.get(\"NNP\", 21), chunk_tags.get(\"B-NP\", 11), ner_tags[\"B-MINES\"]])\n",
    "                for j in range(1, len(found_mine)):\n",
    "                    tokens_data.append([found_mine[j], pos_tags.get(\"NNP\", 21), chunk_tags.get(\"I-NP\", 12), ner_tags[\"I-MINES\"]])\n",
    "                \n",
    "                i += len(found_mine)\n",
    "                continue\n",
    "            \n",
    "            # Checking for metals\n",
    "            if token in metals:\n",
    "                tokens_data.append([token, pos_tags.get(\"NN\", 21), chunk_tags.get(\"B-NP\", 11), ner_tags[\"B-METALS\"]])\n",
    "            else:\n",
    "                # If the token is neither a mine nor a metal\n",
    "                tokens_data.append([token, pos_tags.get(\"NN\", 21), chunk_tags[\"O\"], ner_tags[\"O\"]])\n",
    "            \n",
    "            i += 1\n",
    "        \n",
    "        data.append(tokens_data)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Applying the token annotation function\n",
    "tagged_data = tag_tokens(news_list, mines, metals)\n",
    "\n",
    "# Function to save in CoNLL-2003 format\n",
    "def save_to_conll(data, file_name):\n",
    "    with open(file_name, \"w\", encoding=\"utf-8\") as f:\n",
    "        for sentence in data:\n",
    "            for token_data in sentence:\n",
    "                # Convert each element in token_data to a string and join with spaces\n",
    "                f.write(\" \".join(map(str, token_data)) + \"\\n\")\n",
    "            f.write(\"\\n\")  # Separate sentences with an empty line\n",
    "\n",
    "# Saving to file\n",
    "save_to_conll(tagged_data, \"annotated_news_data.conll\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
      "        num_rows: 84\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
      "        num_rows: 18\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
      "        num_rows: 18\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "import random\n",
    "\n",
    "# Function to read data from a CoNLL file and transform it into the required format\n",
    "def load_conll_data(file_path):\n",
    "    dataset = []\n",
    "    sentence = {\n",
    "        \"id\": 0,\n",
    "        \"tokens\": [],\n",
    "        \"pos_tags\": [],\n",
    "        \"chunk_tags\": [],\n",
    "        \"ner_tags\": []\n",
    "    }\n",
    "    \n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                token, pos_tag, chunk_tag, ner_tag = line.split()\n",
    "                sentence[\"tokens\"].append(token)\n",
    "                sentence[\"pos_tags\"].append(pos_tag)\n",
    "                sentence[\"chunk_tags\"].append(chunk_tag)\n",
    "                sentence[\"ner_tags\"].append(int(ner_tag))\n",
    "            else:\n",
    "                # Add the sentence to the dataset\n",
    "                dataset.append(sentence)\n",
    "                # Update the id and reset for the next sentence\n",
    "                sentence = {\n",
    "                    \"id\": len(dataset),\n",
    "                    \"tokens\": [],\n",
    "                    \"pos_tags\": [],\n",
    "                    \"chunk_tags\": [],\n",
    "                    \"ner_tags\": []\n",
    "                }\n",
    "    \n",
    "    # Add the last sentence if the file does not end with an empty line\n",
    "    if sentence[\"tokens\"]:\n",
    "        dataset.append(sentence)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# Load data\n",
    "file_path = \"annotated_news_data.conll\"\n",
    "data = load_conll_data(file_path)\n",
    "\n",
    "# Shuffle the data and split into train, validation, and test\n",
    "random.shuffle(data)\n",
    "train_size = int(0.7 * len(data))\n",
    "valid_size = int(0.15 * len(data))\n",
    "\n",
    "train_data = data[:train_size]\n",
    "validation_data = data[train_size:train_size + valid_size]\n",
    "test_data = data[train_size + valid_size:]\n",
    "\n",
    "# Convert the data into DatasetDict format\n",
    "ner_data = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(pd.DataFrame(train_data)),\n",
    "    \"validation\": Dataset.from_pandas(pd.DataFrame(validation_data)),\n",
    "    \"test\": Dataset.from_pandas(pd.DataFrame(test_data))\n",
    "})\n",
    "\n",
    "# View the structure of DatasetDict\n",
    "print(ner_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 114,\n",
       " 'tokens': ['Falcon',\n",
       "  'Gold',\n",
       "  'Corp',\n",
       "  'announces',\n",
       "  'the',\n",
       "  'development',\n",
       "  'of',\n",
       "  'the',\n",
       "  'ERSA',\n",
       "  'gold',\n",
       "  'mining',\n",
       "  'project',\n",
       "  'in',\n",
       "  'La',\n",
       "  'Rioja',\n",
       "  'Argentina',\n",
       "  'Extensive',\n",
       "  'drilling',\n",
       "  'campaigns',\n",
       "  'have',\n",
       "  'revealed',\n",
       "  'substantial',\n",
       "  'gold',\n",
       "  'copper',\n",
       "  'silver',\n",
       "  'lead',\n",
       "  'zinc',\n",
       "  'and',\n",
       "  'vanadium',\n",
       "  'mineralization',\n",
       "  'indicating',\n",
       "  'the',\n",
       "  'project',\n",
       "  's',\n",
       "  'potential',\n",
       "  'for',\n",
       "  'long',\n",
       "  'term',\n",
       "  'resource',\n",
       "  'extraction',\n",
       "  'Falcon',\n",
       "  'Gold',\n",
       "  'Corp',\n",
       "  'is',\n",
       "  'committed',\n",
       "  'to',\n",
       "  'responsible',\n",
       "  'mining',\n",
       "  'practices',\n",
       "  'and',\n",
       "  'sustainable',\n",
       "  'resource',\n",
       "  'development',\n",
       "  'at',\n",
       "  'the',\n",
       "  'ERSA',\n",
       "  'project'],\n",
       " 'pos_tags': ['21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21'],\n",
       " 'chunk_tags': ['0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '11',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '11',\n",
       "  '11',\n",
       "  '11',\n",
       "  '0',\n",
       "  '11',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0'],\n",
       " 'ner_tags': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  3,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  0,\n",
       "  3,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# structure of train data\n",
    "ner_data['train'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 115, 105, 57, 113, 53, 107, 89, 83, 91, 25, 71, 102, 77, 64, 29, 27, 106]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#structure of test data\n",
    "ner_data['validation']['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 117, 'tokens': [], 'pos_tags': [], 'chunk_tags': [], 'ner_tags': []}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_data['test'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_data['validation'].features['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_data['train'].features[\"ner_tags\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_data['train'].features[\"ner_tags\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = ner_data['train'][0]\n",
    "tokenized_input = tokenizer(example_text['tokens'],is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "word_ids = tokenized_input.word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 102], 'token_type_ids': [0, 0], 'attention_mask': [1, 1]}\n",
      "\n",
      "\n",
      "['[CLS]', '[SEP]']\n",
      "\n",
      "\n",
      "[None, None]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_input)\n",
    "print(\"\\n\")\n",
    "print(tokens)\n",
    "print(\"\\n\")\n",
    "print(word_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the tokens is : 2\n",
      "Length of the ner tags is: 0\n"
     ]
    }
   ],
   "source": [
    "print(f'Length of the tokens is : {len(tokens)}')\n",
    "print(f'Length of the ner tags is: {len(ner_data[\"train\"][0][\"ner_tags\"])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples, label_all_tokens=True):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        # word_ids() => Return a list mapping the tokens\n",
    "        # to their actual word in the initial sentence.\n",
    "        # It Returns a list indicating the word corresponding to each token.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        # Special tokens like `` and `<\\s>` are originally mapped to None\n",
    "        # We need to set the label to -100 so they are automatically ignored in the loss function.\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                # set â€“100 as the label for these special tokens\n",
    "                label_ids.append(-100)\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                # if current word_idx is != prev then its the most regular case\n",
    "                # and add the corresponding token\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                # to take care of sub-words which have the same word_idx\n",
    "                # set -100 as well for them, but only if label_all_tokens == False\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "                # mask the subword representations after the first subword\n",
    "\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': [84],\n",
       " 'tokens': [['Condoryacu',\n",
       "   'S',\n",
       "   'R',\n",
       "   'L',\n",
       "   'reports',\n",
       "   'progress',\n",
       "   'on',\n",
       "   'the',\n",
       "   'Condoryacu',\n",
       "   'gold',\n",
       "   'project',\n",
       "   'in',\n",
       "   'Catamarca',\n",
       "   'Argentina',\n",
       "   'Recent',\n",
       "   'drilling',\n",
       "   'results',\n",
       "   'have',\n",
       "   'confirmed',\n",
       "   'the',\n",
       "   'presence',\n",
       "   'of',\n",
       "   'gold',\n",
       "   'silver',\n",
       "   'and',\n",
       "   'copper',\n",
       "   'mineralization',\n",
       "   'supporting',\n",
       "   'the',\n",
       "   'project',\n",
       "   's',\n",
       "   'economic',\n",
       "   'feasibility',\n",
       "   'Condoryacu',\n",
       "   'S',\n",
       "   'R',\n",
       "   'L',\n",
       "   'aims',\n",
       "   'to',\n",
       "   'advance',\n",
       "   'the',\n",
       "   'project',\n",
       "   'towards',\n",
       "   'development',\n",
       "   'in',\n",
       "   'the',\n",
       "   'near',\n",
       "   'future']],\n",
       " 'pos_tags': [['21',\n",
       "   '21',\n",
       "   '21',\n",
       "   '21',\n",
       "   '21',\n",
       "   '21',\n",
       "   '21',\n",
       "   '21',\n",
       "   '21',\n",
       "   '21',\n",
       "   '21',\n",
       "   '21',\n",
       "   '21',\n",
       "   '21',\n",
       "   '21',\n",
       "   '21',\n",
       "   '21',\n",
       "   '21',\n",
       "   '21',\n",
       "   '21',\n",
       "   '21',\n",
       "   '21',\n",
       "   '21',\n",
       "   '21',\n",
       "   '21',\n",
       "   '21',\n",
       "   '21',\n",
       "   '21',\n",
       "   '21',\n",
       "   '21',\n",
       "   '21',\n",
       "   '21',\n",
       "   '21',\n",
       "   '21',\n",
       "   '21',\n",
       "   '21',\n",
       "   '21',\n",
       "   '21',\n",
       "   '21',\n",
       "   '21',\n",
       "   '21',\n",
       "   '21',\n",
       "   '21',\n",
       "   '21',\n",
       "   '21',\n",
       "   '21',\n",
       "   '21',\n",
       "   '21']],\n",
       " 'chunk_tags': [['0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '11',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '11',\n",
       "   '11',\n",
       "   '0',\n",
       "   '11',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0',\n",
       "   '0']],\n",
       " 'ner_tags': [[0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   3,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   3,\n",
       "   3,\n",
       "   0,\n",
       "   3,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0]]}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_data['train'][1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 29260, 3148, 10841, 1055, 1054, 1048, 4311, 5082, 2006, 1996, 29260, 3148, 10841, 2751, 2622, 1999, 4937, 8067, 18992, 5619, 3522, 15827, 3463, 2031, 4484, 1996, 3739, 1997, 2751, 3165, 1998, 6967, 9754, 3989, 4637, 1996, 2622, 1055, 3171, 24010, 29260, 3148, 10841, 1055, 1054, 1048, 8704, 2000, 5083, 1996, 2622, 2875, 2458, 1999, 1996, 2379, 2925, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]]}\n"
     ]
    }
   ],
   "source": [
    "q = tokenize_and_align_labels(ner_data['train'][1:2])\n",
    "print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]___________________________________ -100\n",
      "condor__________________________________ 0\n",
      "##ya____________________________________ 0\n",
      "##cu____________________________________ 0\n",
      "s_______________________________________ 0\n",
      "r_______________________________________ 0\n",
      "l_______________________________________ 0\n",
      "reports_________________________________ 0\n",
      "progress________________________________ 0\n",
      "on______________________________________ 0\n",
      "the_____________________________________ 0\n",
      "condor__________________________________ 0\n",
      "##ya____________________________________ 0\n",
      "##cu____________________________________ 0\n",
      "gold____________________________________ 3\n",
      "project_________________________________ 0\n",
      "in______________________________________ 0\n",
      "cat_____________________________________ 0\n",
      "##ama___________________________________ 0\n",
      "##rca___________________________________ 0\n",
      "argentina_______________________________ 0\n",
      "recent__________________________________ 0\n",
      "drilling________________________________ 0\n",
      "results_________________________________ 0\n",
      "have____________________________________ 0\n",
      "confirmed_______________________________ 0\n",
      "the_____________________________________ 0\n",
      "presence________________________________ 0\n",
      "of______________________________________ 0\n",
      "gold____________________________________ 3\n",
      "silver__________________________________ 3\n",
      "and_____________________________________ 0\n",
      "copper__________________________________ 3\n",
      "mineral_________________________________ 0\n",
      "##ization_______________________________ 0\n",
      "supporting______________________________ 0\n",
      "the_____________________________________ 0\n",
      "project_________________________________ 0\n",
      "s_______________________________________ 0\n",
      "economic________________________________ 0\n",
      "feasibility_____________________________ 0\n",
      "condor__________________________________ 0\n",
      "##ya____________________________________ 0\n",
      "##cu____________________________________ 0\n",
      "s_______________________________________ 0\n",
      "r_______________________________________ 0\n",
      "l_______________________________________ 0\n",
      "aims____________________________________ 0\n",
      "to______________________________________ 0\n",
      "advance_________________________________ 0\n",
      "the_____________________________________ 0\n",
      "project_________________________________ 0\n",
      "towards_________________________________ 0\n",
      "development_____________________________ 0\n",
      "in______________________________________ 0\n",
      "the_____________________________________ 0\n",
      "near____________________________________ 0\n",
      "future__________________________________ 0\n",
      "[SEP]___________________________________ -100\n"
     ]
    }
   ],
   "source": [
    "for token, label in zip(tokenizer.convert_ids_to_tokens(q[\"input_ids\"][0]),q[\"labels\"][0]):\n",
    "    print(f\"{token:_<40} {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:00<00:00, 4792.45 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:00<00:00, 3474.02 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:00<00:00, 3357.53 examples/s]\n"
     ]
    }
   ],
   "source": [
    "## Applying on entire data\n",
    "tokenized_datasets = ner_data.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 84,\n",
       " 'tokens': ['Condoryacu',\n",
       "  'S',\n",
       "  'R',\n",
       "  'L',\n",
       "  'reports',\n",
       "  'progress',\n",
       "  'on',\n",
       "  'the',\n",
       "  'Condoryacu',\n",
       "  'gold',\n",
       "  'project',\n",
       "  'in',\n",
       "  'Catamarca',\n",
       "  'Argentina',\n",
       "  'Recent',\n",
       "  'drilling',\n",
       "  'results',\n",
       "  'have',\n",
       "  'confirmed',\n",
       "  'the',\n",
       "  'presence',\n",
       "  'of',\n",
       "  'gold',\n",
       "  'silver',\n",
       "  'and',\n",
       "  'copper',\n",
       "  'mineralization',\n",
       "  'supporting',\n",
       "  'the',\n",
       "  'project',\n",
       "  's',\n",
       "  'economic',\n",
       "  'feasibility',\n",
       "  'Condoryacu',\n",
       "  'S',\n",
       "  'R',\n",
       "  'L',\n",
       "  'aims',\n",
       "  'to',\n",
       "  'advance',\n",
       "  'the',\n",
       "  'project',\n",
       "  'towards',\n",
       "  'development',\n",
       "  'in',\n",
       "  'the',\n",
       "  'near',\n",
       "  'future'],\n",
       " 'pos_tags': ['21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21',\n",
       "  '21'],\n",
       " 'chunk_tags': ['0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '11',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '11',\n",
       "  '11',\n",
       "  '0',\n",
       "  '11',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '0'],\n",
       " 'ner_tags': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  3,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  3,\n",
       "  3,\n",
       "  0,\n",
       "  3,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'input_ids': [101,\n",
       "  29260,\n",
       "  3148,\n",
       "  10841,\n",
       "  1055,\n",
       "  1054,\n",
       "  1048,\n",
       "  4311,\n",
       "  5082,\n",
       "  2006,\n",
       "  1996,\n",
       "  29260,\n",
       "  3148,\n",
       "  10841,\n",
       "  2751,\n",
       "  2622,\n",
       "  1999,\n",
       "  4937,\n",
       "  8067,\n",
       "  18992,\n",
       "  5619,\n",
       "  3522,\n",
       "  15827,\n",
       "  3463,\n",
       "  2031,\n",
       "  4484,\n",
       "  1996,\n",
       "  3739,\n",
       "  1997,\n",
       "  2751,\n",
       "  3165,\n",
       "  1998,\n",
       "  6967,\n",
       "  9754,\n",
       "  3989,\n",
       "  4637,\n",
       "  1996,\n",
       "  2622,\n",
       "  1055,\n",
       "  3171,\n",
       "  24010,\n",
       "  29260,\n",
       "  3148,\n",
       "  10841,\n",
       "  1055,\n",
       "  1054,\n",
       "  1048,\n",
       "  8704,\n",
       "  2000,\n",
       "  5083,\n",
       "  1996,\n",
       "  2622,\n",
       "  2875,\n",
       "  2458,\n",
       "  1999,\n",
       "  1996,\n",
       "  2379,\n",
       "  2925,\n",
       "  102],\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'labels': [-100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  3,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  3,\n",
       "  3,\n",
       "  0,\n",
       "  3,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100]}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets['train'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Defining model\n",
    "ner_model = AutoModelForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('4.46.1', '1.1.0')"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install accelerate>=1\n",
    "import accelerate\n",
    "import transformers\n",
    "\n",
    "transformers.__version__, accelerate.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tf-keras in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (2.18.0)\n",
      "Requirement already satisfied: tensorflow<2.19,>=2.18 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from tf-keras) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: packaging in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (5.28.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (1.67.1)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (3.6.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (2.0.2)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (0.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from tensorflow<2.19,>=2.18->tf-keras) (0.37.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow<2.19,>=2.18->tf-keras) (0.38.4)\n",
      "Requirement already satisfied: rich in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (13.9.3)\n",
      "Requirement already satisfied: namex in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (0.0.8)\n",
      "Requirement already satisfied: optree in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (0.13.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow<2.19,>=2.18->tf-keras) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (3.0.6)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (8.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (2.18.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.19,>=2.18->tensorflow<2.19,>=2.18->tf-keras) (3.20.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.19,>=2.18->tf-keras) (0.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages/torch/cuda/__init__.py:716: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install tf-keras\n",
    "#Define training args\n",
    "from transformers import TrainingArguments, Trainer\n",
    "args = TrainingArguments(\n",
    "\"test-ner\",\n",
    "evaluation_strategy = \"epoch\",\n",
    "learning_rate=2e-5,\n",
    "per_device_train_batch_size=32,\n",
    "per_device_eval_batch_size=32,\n",
    "num_train_epochs=20,\n",
    "weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U datasets evaluate\n",
    "from evaluate import load\n",
    "metric = load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value(dtype='int64', id=None)\n"
     ]
    }
   ],
   "source": [
    "print(ner_data[\"train\"].features[\"ner_tags\"].feature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Casting the dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84/84 [00:00<00:00, 10128.84 examples/s]\n",
      "Casting the dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:00<00:00, 3748.63 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Casting the dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:00<00:00, 3993.52 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'B-MINES', 'I-MINES', 'B-METALS']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['O', 'B-MINES', 'I-MINES', 'B-METALS']"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import DatasetDict, ClassLabel, Sequence, Features, Value\n",
    "\n",
    "# Defining labels for ner_tags\n",
    "ner_label_names = ['O', 'B-MINES', 'I-MINES', 'B-METALS']\n",
    "features = Features({\n",
    "    \"id\": Value(\"int64\"),\n",
    "    \"tokens\": Sequence(Value(\"string\")),\n",
    "    \"pos_tags\": Sequence(Value(\"int64\")),\n",
    "    \"chunk_tags\": Sequence(Value(\"int64\")),\n",
    "    \"ner_tags\": Sequence(ClassLabel(names=ner_label_names))\n",
    "})\n",
    "\n",
    "# Converting ner_tags to ClassLabel after dataset creation\n",
    "ner_data[\"train\"] = ner_data[\"train\"].cast(features)\n",
    "ner_data[\"validation\"] = ner_data[\"validation\"].cast(features)\n",
    "ner_data[\"test\"] = ner_data[\"test\"].cast(features)\n",
    "\n",
    "# Now checking the labels\n",
    "print(ner_data[\"train\"].features[\"ner_tags\"].feature.names)\n",
    "label_list = ner_data[\"train\"].features[\"ner_tags\"].feature.names\n",
    "label_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    pred_logits, labels = eval_preds\n",
    "    print(eval_preds)\n",
    "\n",
    "    pred_logits = np.argmax(pred_logits, axis=2)\n",
    "    # the logits and the probabilities are in the same order,\n",
    "    # so we donâ€™t need to apply the softmax\n",
    "\n",
    "    # We remove all the values where the label is -100\n",
    "    predictions = [\n",
    "        [label_list[eval_preds] for (eval_preds, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(pred_logits, labels)\n",
    "    ]\n",
    "\n",
    "    true_labels = [\n",
    "      [label_list[l] for (eval_preds, l) in zip(prediction, label) if l != -100]\n",
    "       for prediction, label in zip(pred_logits, labels)\n",
    "   ]\n",
    "    results = metric.compute(predictions=predictions, references=true_labels)\n",
    "\n",
    "    return {\n",
    "          \"precision\": results[\"overall_precision\"],\n",
    "          \"recall\": results[\"overall_recall\"],\n",
    "          \"f1\": results[\"overall_f1\"],\n",
    "          \"accuracy\": results[\"overall_accuracy\"],\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_574865/2818633497.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "   ner_model,\n",
    "   args,\n",
    "   train_dataset=tokenized_datasets[\"train\"],\n",
    "   eval_dataset=tokenized_datasets[\"validation\"],\n",
    "   data_collator=data_collator,\n",
    "   tokenizer=tokenizer,\n",
    "   compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|â–Œ         | 3/60 [00:10<03:17,  3.47s/it]/home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/swiftx/miniconda3/envs/tf/lib/python3.9/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      "  5%|â–Œ         | 3/60 [00:11<03:17,  3.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x754052529670>\n",
      "{'eval_loss': 0.6635697484016418, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.8363636363636363, 'eval_runtime': 0.6708, 'eval_samples_per_second': 26.833, 'eval_steps_per_second': 1.491, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                              \n",
      " 10%|â–ˆ         | 6/60 [00:24<03:26,  3.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x754053f1a3a0>\n",
      "{'eval_loss': 0.5620435476303101, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.8363636363636363, 'eval_runtime': 0.8122, 'eval_samples_per_second': 22.163, 'eval_steps_per_second': 1.231, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                              \n",
      " 15%|â–ˆâ–Œ        | 9/60 [00:38<03:35,  4.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x7541b1823670>\n",
      "{'eval_loss': 0.4971984326839447, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_accuracy': 0.8363636363636363, 'eval_runtime': 0.8037, 'eval_samples_per_second': 22.396, 'eval_steps_per_second': 1.244, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 20%|â–ˆâ–ˆ        | 12/60 [00:52<03:28,  4.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x754053f6a670>\n",
      "{'eval_loss': 0.40264856815338135, 'eval_precision': 1.0, 'eval_recall': 0.25925925925925924, 'eval_f1': 0.4117647058823529, 'eval_accuracy': 0.8681818181818182, 'eval_runtime': 0.7999, 'eval_samples_per_second': 22.504, 'eval_steps_per_second': 1.25, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 25%|â–ˆâ–ˆâ–Œ       | 15/60 [01:06<03:15,  4.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x7541b1831670>\n",
      "{'eval_loss': 0.3450518846511841, 'eval_precision': 0.8235294117647058, 'eval_recall': 0.5185185185185185, 'eval_f1': 0.6363636363636364, 'eval_accuracy': 0.8863636363636364, 'eval_runtime': 0.8135, 'eval_samples_per_second': 22.126, 'eval_steps_per_second': 1.229, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 30%|â–ˆâ–ˆâ–ˆ       | 18/60 [01:21<03:07,  4.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x7541b1836040>\n",
      "{'eval_loss': 0.3024390637874603, 'eval_precision': 0.7368421052631579, 'eval_recall': 0.5185185185185185, 'eval_f1': 0.6086956521739131, 'eval_accuracy': 0.8772727272727273, 'eval_runtime': 0.8055, 'eval_samples_per_second': 22.347, 'eval_steps_per_second': 1.241, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 21/60 [01:35<02:54,  4.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x7541b18360d0>\n",
      "{'eval_loss': 0.2680642902851105, 'eval_precision': 0.7894736842105263, 'eval_recall': 0.5555555555555556, 'eval_f1': 0.6521739130434783, 'eval_accuracy': 0.8863636363636364, 'eval_runtime': 0.7872, 'eval_samples_per_second': 22.867, 'eval_steps_per_second': 1.27, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 24/60 [01:50<02:40,  4.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x7541b18360d0>\n",
      "{'eval_loss': 0.2355039417743683, 'eval_precision': 0.8, 'eval_recall': 0.5925925925925926, 'eval_f1': 0.6808510638297872, 'eval_accuracy': 0.8909090909090909, 'eval_runtime': 0.8213, 'eval_samples_per_second': 21.916, 'eval_steps_per_second': 1.218, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 27/60 [02:04<02:27,  4.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x7540725e8730>\n",
      "{'eval_loss': 0.2031565010547638, 'eval_precision': 0.782608695652174, 'eval_recall': 0.6666666666666666, 'eval_f1': 0.72, 'eval_accuracy': 0.9136363636363637, 'eval_runtime': 0.8176, 'eval_samples_per_second': 22.016, 'eval_steps_per_second': 1.223, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 30/60 [02:18<02:11,  4.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x75406b2a5130>\n",
      "{'eval_loss': 0.17096249759197235, 'eval_precision': 0.7142857142857143, 'eval_recall': 0.7407407407407407, 'eval_f1': 0.7272727272727273, 'eval_accuracy': 0.9227272727272727, 'eval_runtime': 0.9094, 'eval_samples_per_second': 19.793, 'eval_steps_per_second': 1.1, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 33/60 [02:33<02:03,  4.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x7540533f8700>\n",
      "{'eval_loss': 0.1510426104068756, 'eval_precision': 0.7407407407407407, 'eval_recall': 0.7407407407407407, 'eval_f1': 0.7407407407407407, 'eval_accuracy': 0.9272727272727272, 'eval_runtime': 0.8822, 'eval_samples_per_second': 20.402, 'eval_steps_per_second': 1.133, 'epoch': 11.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 36/60 [02:49<01:55,  4.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x7540533c1dc0>\n",
      "{'eval_loss': 0.1329496055841446, 'eval_precision': 0.7241379310344828, 'eval_recall': 0.7777777777777778, 'eval_f1': 0.75, 'eval_accuracy': 0.9454545454545454, 'eval_runtime': 0.9151, 'eval_samples_per_second': 19.669, 'eval_steps_per_second': 1.093, 'epoch': 12.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 39/60 [03:08<01:52,  5.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x7540533e2dc0>\n",
      "{'eval_loss': 0.11756303906440735, 'eval_precision': 0.75, 'eval_recall': 0.7777777777777778, 'eval_f1': 0.7636363636363638, 'eval_accuracy': 0.9590909090909091, 'eval_runtime': 1.3079, 'eval_samples_per_second': 13.763, 'eval_steps_per_second': 0.765, 'epoch': 13.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 42/60 [03:29<01:51,  6.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x7541b1817ee0>\n",
      "{'eval_loss': 0.10601700842380524, 'eval_precision': 0.75, 'eval_recall': 0.7777777777777778, 'eval_f1': 0.7636363636363638, 'eval_accuracy': 0.9590909090909091, 'eval_runtime': 1.2862, 'eval_samples_per_second': 13.995, 'eval_steps_per_second': 0.778, 'epoch': 14.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 45/60 [03:48<01:30,  6.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x7541b1817e80>\n",
      "{'eval_loss': 0.09762444347143173, 'eval_precision': 0.75, 'eval_recall': 0.7777777777777778, 'eval_f1': 0.7636363636363638, 'eval_accuracy': 0.9590909090909091, 'eval_runtime': 0.7845, 'eval_samples_per_second': 22.945, 'eval_steps_per_second': 1.275, 'epoch': 15.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 48/60 [04:02<01:00,  5.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x7540b8ea97f0>\n",
      "{'eval_loss': 0.09221401810646057, 'eval_precision': 0.8, 'eval_recall': 0.8888888888888888, 'eval_f1': 0.8421052631578948, 'eval_accuracy': 0.9681818181818181, 'eval_runtime': 0.8455, 'eval_samples_per_second': 21.288, 'eval_steps_per_second': 1.183, 'epoch': 16.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 51/60 [04:17<00:41,  4.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x7540b8ea97f0>\n",
      "{'eval_loss': 0.08991530537605286, 'eval_precision': 0.78125, 'eval_recall': 0.9259259259259259, 'eval_f1': 0.847457627118644, 'eval_accuracy': 0.9681818181818181, 'eval_runtime': 0.7889, 'eval_samples_per_second': 22.817, 'eval_steps_per_second': 1.268, 'epoch': 17.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 54/60 [04:31<00:27,  4.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x75406b3edaf0>\n",
      "{'eval_loss': 0.08807323127985, 'eval_precision': 0.78125, 'eval_recall': 0.9259259259259259, 'eval_f1': 0.847457627118644, 'eval_accuracy': 0.9681818181818181, 'eval_runtime': 0.8491, 'eval_samples_per_second': 21.199, 'eval_steps_per_second': 1.178, 'epoch': 18.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 57/60 [04:46<00:13,  4.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x754053f05040>\n",
      "{'eval_loss': 0.08628478646278381, 'eval_precision': 0.78125, 'eval_recall': 0.9259259259259259, 'eval_f1': 0.847457627118644, 'eval_accuracy': 0.9681818181818181, 'eval_runtime': 0.8969, 'eval_samples_per_second': 20.07, 'eval_steps_per_second': 1.115, 'epoch': 19.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60/60 [05:03<00:00,  5.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x7541b4c35f40>\n",
      "{'eval_loss': 0.08527182042598724, 'eval_precision': 0.78125, 'eval_recall': 0.9259259259259259, 'eval_f1': 0.847457627118644, 'eval_accuracy': 0.9727272727272728, 'eval_runtime': 0.7351, 'eval_samples_per_second': 24.485, 'eval_steps_per_second': 1.36, 'epoch': 20.0}\n",
      "{'train_runtime': 303.1621, 'train_samples_per_second': 5.542, 'train_steps_per_second': 0.198, 'train_loss': 0.18968644142150878, 'epoch': 20.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=60, training_loss=0.18968644142150878, metrics={'train_runtime': 303.1621, 'train_samples_per_second': 5.542, 'train_steps_per_second': 0.198, 'total_flos': 59474666884416.0, 'train_loss': 0.18968644142150878, 'epoch': 20.0})"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save model\n",
    "ner_model.save_pretrained(\"ner_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tokenizer/tokenizer_config.json',\n",
       " 'tokenizer/special_tokens_map.json',\n",
       " 'tokenizer/vocab.txt',\n",
       " 'tokenizer/added_tokens.json',\n",
       " 'tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Save tokenizer\n",
    "tokenizer.save_pretrained(\"tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    str(i): label for i,label in enumerate(label_list)\n",
    "}\n",
    "label2id = {\n",
    "    label: str(i) for i,label in enumerate(label_list)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': 'O', '1': 'B-MINES', '2': 'I-MINES', '3': 'B-METALS'}"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': '0', 'B-MINES': '1', 'I-MINES': '2', 'B-METALS': '3'}"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = json.load(open(\"ner_model/config.json\"))\n",
    "config[\"id2label\"] = id2label\n",
    "config[\"label2id\"] = label2id\n",
    "json.dump(config, open(\"ner_model/config.json\",\"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fine_tuned = AutoModelForTokenClassification.from_pretrained(\"ner_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'B-MINES', 'score': np.float32(0.6271148), 'index': 1, 'word': 'bo', 'start': 0, 'end': 2}, {'entity': 'B-MINES', 'score': np.float32(0.5719211), 'index': 2, 'word': '##lad', 'start': 2, 'end': 5}, {'entity': 'B-MINES', 'score': np.float32(0.50259656), 'index': 5, 'word': 'cn', 'start': 13, 'end': 15}, {'entity': 'B-MINES', 'score': np.float32(0.5168247), 'index': 6, 'word': '##ado', 'start': 15, 'end': 18}]\n"
     ]
    }
   ],
   "source": [
    "nlp = pipeline(\"ner\",model=model_fine_tuned,tokenizer=tokenizer)\n",
    "example = \"Boladora and Cnadon Langostur known in the precious metals sector.\"\n",
    "ner_results = nlp(example)\n",
    "print(ner_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ð¡ÑƒÑ‰Ð½Ð¾ÑÑ‚ÑŒ: bolad, Ð¢ÐµÐ³: B-MINES, ÐÐ°Ñ‡Ð°Ð»Ð¾: 0, ÐšÐ¾Ð½ÐµÑ†: 12, ÐžÑ†ÐµÐ½ÐºÐ°: 0.60\n",
      "Ð¡ÑƒÑ‰Ð½Ð¾ÑÑ‚ÑŒ: cnado, Ð¢ÐµÐ³: B-MINES, ÐÐ°Ñ‡Ð°Ð»Ð¾: 13, ÐšÐ¾Ð½ÐµÑ†: 18, ÐžÑ†ÐµÐ½ÐºÐ°: 0.51\n"
     ]
    }
   ],
   "source": [
    "# Processing and merging subwords with score addition\n",
    "entities = []\n",
    "current_entity = \"\"\n",
    "current_tag = None\n",
    "current_start = None\n",
    "current_score_sum = 0.0\n",
    "current_score_count = 0\n",
    "\n",
    "for item in ner_results:\n",
    "    word = item['word']\n",
    "    tag = item['entity']\n",
    "    score = item['score']\n",
    "    \n",
    "    # Start of a new word or entity\n",
    "    if word.startswith(\"##\"):\n",
    "        # Remove \"##\" and merge with the current word\n",
    "        current_entity += word[2:]\n",
    "        current_score_sum += score\n",
    "        current_score_count += 1\n",
    "    else:\n",
    "        # Save the current entity if it's completed\n",
    "        if current_entity:\n",
    "            avg_score = current_score_sum / current_score_count if current_score_count > 0 else 0\n",
    "            entities.append({\n",
    "                \"entity\": current_tag,\n",
    "                \"word\": current_entity,\n",
    "                \"start\": current_start,\n",
    "                \"end\": item['start'] - 1,  # End of the previous word\n",
    "                \"score\": avg_score\n",
    "            })\n",
    "        \n",
    "        # Initialize a new word\n",
    "        current_entity = word\n",
    "        current_tag = tag\n",
    "        current_start = item['start']\n",
    "        current_score_sum = score\n",
    "        current_score_count = 1\n",
    "\n",
    "# Add the last entity\n",
    "if current_entity:\n",
    "    avg_score = current_score_sum / current_score_count if current_score_count > 0 else 0\n",
    "    entities.append({\n",
    "        \"entity\": current_tag,\n",
    "        \"word\": current_entity,\n",
    "        \"start\": current_start,\n",
    "        \"end\": item['end'],\n",
    "        \"score\": avg_score\n",
    "    })\n",
    "\n",
    "# Output the merged entities\n",
    "for entity in entities:\n",
    "    print(f\"Entity: {entity['word']}, Tag: {entity['entity']}, Start: {entity['start']}, End: {entity['end']}, Score: {entity['score']:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
